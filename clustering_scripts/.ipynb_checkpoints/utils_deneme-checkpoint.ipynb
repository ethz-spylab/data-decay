{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1e6cf0-54c7-46c6-976d-226bdc3b174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdbcf4e-4500-4d83-991c-c6916ed5069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_FOLDER = Path(\"/data/cc3m\")\n",
    "EMBEDDINGS_FOLDER = DATA_FOLDER / \"cc3m_2023/embeddings\"\n",
    "IMAGENET_EMBEDDINGS_FOLDER = EMBEDDINGS_FOLDER / \"imagenet_class_embeddings_L14.npy\"\n",
    "CC_EMBEDDINGS_FOLDER = EMBEDDINGS_FOLDER / \"text_embeddings_L14.npy\"\n",
    "CC_CAPTIONS_DF = \"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\"\n",
    "DOT_PRODUCTS = EMBEDDINGS_FOLDER / \"CC_vs_imagenet_L14.npy\"  # (cc entries, imagenet classes)\n",
    "IMAGENET_LABELS = DATA_FOLDER / \"imagenet_classes.txt\"\n",
    "LONG_IMAGENET_LABELS = DATA_FOLDER / \"imagenet_classes_long.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bde0912-edbb-4e71-9565-8d82dcdf21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_products = np.load(DOT_PRODUCTS)\n",
    "argmax = np.argmax(dot_products, axis=1)\n",
    "maxes = np.max(dot_products, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de2dc981-8046-4ad6-a4bf-4ebbbb6dcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_captions(similarity: np.ndarray, \n",
    "                          col_id : int, \n",
    "                          n_relevant = 10,\n",
    "                          only_argmax = False,\n",
    "                          sort_best = False, \n",
    "                          CAPTIONS_FILE = \"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\"):\n",
    "    \"\"\"\n",
    "    Get the relevant captions for a given column of the dot products matrix.\n",
    "    Args:\n",
    "        similarity: (cc entries, classes) matrix. Each row corresponds to a CC entry and each column to a class.o\n",
    "                    The entries are similarity values between the CC entry and the class.\n",
    "                    Example: dot product matrix.\n",
    "                    Example: 1 - distance matrix\n",
    "        col_id: The column to get the relevant captions for.\n",
    "        n_relevant: The number of relevant captions to return.\n",
    "        only_argmax: If True, only consider the captions most similar to given col. If False, consider all captions.\n",
    "        sort_best: If True, return the top n_relevant similarities. If False, choose randomly.\n",
    "    \n",
    "    Return:\n",
    "        A list of n_relevant captions.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO take a look at argmax_check.py file for inspiration\n",
    "    \n",
    "    \n",
    "    captions = pd.read_csv(CAPTIONS_FILE, sep='\\t', names=[\"caption\",\"url\"], usecols=range(0,2))[\"caption\"].tolist()\n",
    "    assert similarity.shape[0] == len(captions), \"Similarity matrix and captions length do not match!\"\n",
    "    assert similarity.shape[1] - 1 >= abs(col_id), \"col_id exceeds the # columns in similarity matrix!\"\n",
    "    similarity_relevant = similarity[:,col_id]\n",
    "    if only_argmax == True:\n",
    "        argmax = np.argmax(similarity, axis=1)\n",
    "        maxes = np.max(similarity, axis=1)\n",
    "        similarity_relevant = similarity_relevant[argmax==col_id]\n",
    "        captions = [captions[i] for i in np.where(argmax==col_id)[0]]\n",
    "                              \n",
    "    n_relevant_available = min(n_relevant, len(similarity_relevant))\n",
    "                              \n",
    "    if sort_best != True:\n",
    "        random_entries = random.sample(range(len(similarity_relevant)), n_relevant_available)\n",
    "        return [captions[entry] for entry in random_entries]\n",
    "    else:\n",
    "        idx = np.argpartition(similarity_relevant, -n_relevant_available)[-n_relevant_available:]\n",
    "        idx_sorted = idx[np.argsort(similarity_relevant[idx])][::-1]\n",
    "        return [captions[entry] for entry in idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bdc8c27-9731-4152-8555-34b092f78470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close up of a tarantula',\n",
       " '... and the tarantula that we found on',\n",
       " 'image of a common tarantula .',\n",
       " 'largest species of tarantula in the world',\n",
       " 'he was like a tarantula to me',\n",
       " 'look at this scary spider',\n",
       " 'tarantula , a wolf spider that hunts whilst walking on the ground .',\n",
       " 'tarantula , a wolf spider that hunts whilst walking on the ground .',\n",
       " \"harmless : despite being as big as a man 's hand this particular species of tarantula is completely harmless to humans\",\n",
       " 'tarantula email this to a friend']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relevant_captions(similarity=dot_products, col_id=76, only_argmax=False, sort_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fcb635-b7f5-45d9-9647-1b806ff323dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\", sep='\\t', names=[\"caption\",\"url\"], usecols=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7af053-5c84-494c-a2bb-de4839511a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = data[\"caption\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21ae15d-f89a-4ea6-8af3-b0485af40f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_relevant_available = 10\n",
    "similarity = dot_products\n",
    "col_id = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d28803-7246-4ec8-a30f-7e30c62d5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_entries = random.sample(range(len(similarity[:,col_id])), n_relevant_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efa4032-7ec5-46e3-8073-3c17271414fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the gray cat sleeps on the white sofa',\n",
       " 'sculpture with an umbrella , arboretum .',\n",
       " 'teacher helping kids in a preschool class',\n",
       " 'industry for a city showing the private swimming pool',\n",
       " 'caucasian man preparing to drive the car .',\n",
       " 'a family had homes damaged and cars ruined by hurricane sandy .',\n",
       " 'what does tv character from comic book series look like with no mask ?',\n",
       " 'background with woman in style .',\n",
       " \"football player celebrates scoring his side 's first goal of the game in front of the fans with football player\",\n",
       " 'actor and person attend 25th anniversary event']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[captions[entry] for entry in random_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbc4e04-bb15-4987-bb0c-2d7595f94909",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argpartition(similarity[:,col_id], -n_relevant_available)[-n_relevant_available:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02cf21c6-9170-4330-b2f9-f87da7250692",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sorted = idx[np.argsort(similarity[:,col_id][idx])][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb31853-774d-4a76-8586-f683c3b73511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1233486, 1968039, 1085264, 2798738, 2974413, 2071905, 2928383,\n",
       "        777160, 2140310, 1742130])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beada90e-47b0-4202-ac34-941a20b25d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65565443, 0.6306698 , 0.6259816 , 0.62330246, 0.6229262 ,\n",
       "       0.6229262 , 0.6229262 , 0.6229262 , 0.6229262 , 0.6229262 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity[:,col_id][idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4237143d-22f7-4e8a-bd8a-8c66e2b4c121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a mountain yellow - legged frog .',\n",
       " 'common frog is a widespread species .',\n",
       " 'frog , critically endangered in the wild',\n",
       " 'close view of a common frog',\n",
       " 'this large aquatic frog is organism .',\n",
       " 'this large aquatic frog is organism .',\n",
       " 'this large aquatic frog is organism .',\n",
       " 'this large aquatic frog is organism .',\n",
       " 'this large aquatic frog is organism .',\n",
       " 'this large aquatic frog is organism .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[captions[entry] for entry in idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9009907-606d-4fe7-b32f-91214c9f31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_relevant_available = 10\n",
    "similarity = dot_products\n",
    "col_id = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9337e7-8722-412f-b32b-b11182913bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_relevant = similarity[:,col_id]\n",
    "similarity_relevant = similarity_relevant[argmax==col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f21678dc-99e9-4557-92d4-549a8ca873fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57160926, 0.34483665, 0.4299124 , 0.41132516, 0.52932286,\n",
       "       0.6172392 , 0.5341002 , 0.50568503, 0.5341002 , 0.4778833 ,\n",
       "       0.32334027, 0.5810215 , 0.5038953 , 0.6140211 , 0.5150279 ,\n",
       "       0.40928322, 0.5341917 , 0.5392379 , 0.5898162 , 0.5045848 ,\n",
       "       0.3165427 , 0.42452884, 0.27025238, 0.48669985, 0.4104669 ,\n",
       "       0.47195876, 0.5898162 , 0.30660352, 0.38492748, 0.4178289 ,\n",
       "       0.39712718, 0.2724526 , 0.5045848 , 0.38773084, 0.39452294,\n",
       "       0.2724526 , 0.52399945, 0.3553119 , 0.2724526 , 0.57148767,\n",
       "       0.27025238, 0.3138243 , 0.33028188, 0.53052   , 0.538291  ,\n",
       "       0.3632834 , 0.45262495, 0.2724526 , 0.33028188, 0.5341002 ,\n",
       "       0.56437993], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c89c2d2-2382-4696-b148-c9fb6a418e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_relevant = [captions[i] for i in np.where(argmax==col_id)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "020e4856-d97e-4492-bbfd-1fb55b370cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(LONG_IMAGENET_LABELS).read().splitlines()\n",
    "texts = [\"This is a \" + line for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3475edfe-76cd-4f4d-a0a2-2c42407c35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings = np.load(IMAGENET_EMBEDDINGS_FOLDER)\n",
    "cc_embeddings = np.load(CC_EMBEDDINGS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc919528-e197-400a-b947-7b00fa0104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embeddings_relevant = label_embeddings[col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9ebaf6a-e770-42c0-8178-faa23ccb289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3318333, 768)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(cc_embeddings.shape)\n",
    "print(label_embeddings_relevant.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c46f1aa1-a263-457b-a75b-0e58ab574841",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = label_embeddings_relevant @ cc_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc00906b-94c9-4043-80c1-de7943eff3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison2 = cc_embeddings @ label_embeddings_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bed57430-3251-46bb-81a9-25558ba5fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053526737\n",
      "0.053526737\n",
      "0.053526722\n"
     ]
    }
   ],
   "source": [
    "i = 6787\n",
    "print(comparison[i])\n",
    "print(comparison2[i])\n",
    "print(similarity[i,col_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42337ebf-99f8-470e-abf6-ac7abcacf966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318333,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "323f353a-caf6-436c-9fe4-dd3af2bea326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.ones([3,5])\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d24106c-02c7-43e3-9249-2461cc0a0f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.ones(5)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dffffc78-7ee0-4c95-b4bb-6a5e25c1a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faf63a5e-cd11-4cd5-a15c-acbc52580f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "if a == 1:\n",
    "    print(1)\n",
    "elif a == 3:\n",
    "    print(3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29f08d00-b8ab-47d4-99be-d1dea1005652",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = cc_embeddings - label_embeddings_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fcda41c-c08a-429b-921e-899acdba9b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01856818, -0.04548136, -0.04816247, ...,  0.05884226,\n",
       "         0.07124819, -0.00535386],\n",
       "       [ 0.05407539, -0.06928588, -0.08092396, ...,  0.03008895,\n",
       "        -0.00898384,  0.01607214],\n",
       "       [ 0.04828547, -0.10116106, -0.09367523, ...,  0.05698025,\n",
       "        -0.01283206, -0.05270676],\n",
       "       ...,\n",
       "       [ 0.03008148, -0.07547796,  0.00516605, ...,  0.04212256,\n",
       "         0.03777242, -0.05866101],\n",
       "       [ 0.00449855, -0.04209557, -0.07445265, ...,  0.02327939,\n",
       "         0.0308992 , -0.00660304],\n",
       "       [ 0.02137729, -0.07284716, -0.04025277, ...,  0.03487681,\n",
       "         0.03703548, -0.00302604]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "daf6e337-bd7c-48dc-a017-5f80639cbdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be471de6-114c-4646-b609-ead0fd167860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23606798, 2.23606798, 2.23606798])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(k, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7099794c-9438-44d7-a581-4d981b45b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.linalg.norm(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee849c4d-b60d-492a-ab08-d44db970377c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318333,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d436207-1eb8-41ff-b160-a0438028d590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3594415, 1.3718246, 1.4043311, ..., 1.3754959, 1.3372407,\n",
       "       1.3615527], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44e834a1-57a4-4b5b-bb1d-271211fc5bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8298745"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6319aee4-cda3-4bb4-8566-2157417877ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_captions_from_embeddings(embeddings: np.ndarray, \n",
    "                                          query : np.ndarray,\n",
    "                                          distance_function = \"dot_product\",\n",
    "                                          n_relevant = 10,\n",
    "                                          sort_best = False, \n",
    "                                          CAPTIONS_FILE = \"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\"):\n",
    "    \"\"\"\n",
    "    Get the relevant captions for a given query.\n",
    "    Args:\n",
    "        embeddings: (cc entries, embedding size) matrix. Each row is an embedding for a CC entry.\n",
    "        query: (embedding size,) vector. The query to get the relevant captions for.\n",
    "        distance_function: The distance function to use. Can be \"dot_product\" or \"euclidean\".\n",
    "        n_relevant: The number of relevant captions to return.\n",
    "        sort_best: If True, return the top n_relevant similarities. If False, choose randomly.\n",
    "    \n",
    "    Return:\n",
    "        A list of n_relevant captions.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Compute the similarity\n",
    "    \n",
    "    if distance_function == \"dot_product\":\n",
    "        comparison = embeddings @ query\n",
    "    elif distance_function == \"euclidean\":\n",
    "        diff = embeddings - query\n",
    "        distance = np.linalg.norm(diff, axis=1)\n",
    "        comparison = 1 - distance\n",
    "    else:\n",
    "        raise NotImplementedError(\"This distance method is not implemented yet.\")\n",
    "\n",
    "    # TODO call get_relevant_captions with col_id = 0\n",
    "    return get_relevant_captions(similarity = comparison[:,None], \n",
    "                          col_id = 0, \n",
    "                          n_relevant = n_relevant,\n",
    "                          sort_best = sort_best, \n",
    "                          CAPTIONS_FILE = CAPTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fcac04a7-4562-4b68-a879-b06fec74bf5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_relevant_captions_from_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m76\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdistance_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 26\u001b[0m, in \u001b[0;36mget_relevant_captions_from_embeddings\u001b[0;34m(embeddings, query, distance_function, n_relevant, sort_best, CAPTIONS_FILE)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distance_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m     diff \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m-\u001b[39m query\n\u001b[0;32m---> 26\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     comparison \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m distance\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/local/home/tcelik/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py:2541\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[0;32m-> 2541\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m   2542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_relevant_captions_from_embeddings(cc_embeddings, label_embeddings[76], sort_best=True,distance_function = \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "944ef026-247e-4886-bba5-af123737838a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embeddings_relevant[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1dd672-8c33-4001-8756-6a1b0329762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(3)*3\n",
    "b = np.array([[1,2,3,4,5]]).T*np.ones([5,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c17b633-014e-4e53-bf3a-fc4c62145df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -2., -2.],\n",
       "       [-1., -1., -1.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=b-a\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3a99a2b-9ab5-4ee8-a159-125609f1272e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.46410162, 1.73205081, 0.        , 1.73205081, 3.46410162])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f921f-7d5c-4fba-abee-e3676b9158ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
