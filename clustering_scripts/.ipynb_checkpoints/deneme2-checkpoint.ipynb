{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63dddde8-b346-44a2-a9a6-730c08b2e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/tcelik/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90890aa8-a067-4f4e-aeda-adead369bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/tcelik/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476892f4-6afa-4787-8730-b56c76d199ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb9d2c2-0329-4532-b923-be178708f8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d9086db-715e-4785-9825-5de94ecb72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.text_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e7d508d-247d-4344-a737-d0a1837dca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2166e+00, -5.1482e-01,  4.8002e-01,  ..., -1.3832e-01,\n",
       "          8.1141e-01,  5.5702e-01],\n",
       "        [-9.5111e-01,  7.5051e-03,  1.3867e-01,  ...,  6.0387e-04,\n",
       "          2.2237e-01,  8.9224e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "060a40fa-a3d1-43ce-8824-5ca6190245e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIPModel' object has no attribute 'context_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#input_resolution = model.visual.input_resolution\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m context_length \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_length\u001b[49m\n\u001b[1;32m      3\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[0;32m/local/home/tcelik/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIPModel' object has no attribute 'context_length'"
     ]
    }
   ],
   "source": [
    "#input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ccb302d-6cae-4ef4-b0d1-63307055d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5a6926-d426-4f98-ab2f-0a3aae0c3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\", sep='\\t', names=[\"caption\",\"url\"], usecols=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5019bbad-7abc-42c3-9b81-763c237ed9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sierra looked stunning in this top and this skirt while performing with person at their former university'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['caption'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19078bf0-b9d7-49a8-9711-b6c48e2da6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cybernetic scene isolated on white background .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['caption'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661458ea-1201-4ffe-8f80-2be399fdaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [df_train['caption'][x] for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb189307-eb87-46ac-8d93-8585d2305d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "inputs = processor(text=b, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5030ba06-4151-4db5-a245-49472b78ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05712496500927955"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dbfc65-294c-49f7-b014-8d13cfe344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.perf_counter()\n",
    "    outputs = model.text_model(**inputs)\n",
    "    toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f66123dc-da4b-4c3c-b158-52f8b4872b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.611784199495258"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(toc-tic)*300/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "066a69cb-86fe-450a-9439-949e7f4ac3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████| 4.10k/4.10k [00:00<00:00, 3.17MB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████| 599M/599M [00:05<00:00, 105MB/s]\n",
      "Downloading (…)rocessor_config.json: 100%|████████████████████████████████████████████████████████████| 316/316 [00:00<00:00, 293kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████| 905/905 [00:00<00:00, 593kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|█████████████████████████████████████████████████████████| 961k/961k [00:00<00:00, 2.64MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|█████████████████████████████████████████████████████████| 525k/525k [00:00<00:00, 85.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████| 2.22M/2.22M [00:00<00:00, 4.87MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████| 389/389 [00:00<00:00, 286kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22681dc8-4719-4ee4-a117-24b6f72ce0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=b, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec051b78-7136-48c1-a5eb-0d8b720a01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic2 = time.perf_counter()\n",
    "    outputs = model.text_model(**inputs)\n",
    "    toc2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a20dacd-63ae-4b79-8086-d4495acaf8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.78677717410028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc2-tic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7f3a1-0ad1-48cb-a8aa-a0a6fc89e14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b5d90-522a-4a96-9d95-bbc48ce00cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108dc595-7b3e-49d4-9f10-45ff0f63ba54",
   "metadata": {},
   "source": [
    "Dogrudan Clip modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f64d197-32ef-4ee6-931a-cfd0f0136170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861c2720-e652-4c80-9df3-faf941d75908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6d498f-cc3d-4c75-817e-5203c3292523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda().eval()\n",
    "df_train = pd.read_csv(\"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\", sep='\\t', names=[\"caption\",\"url\"], usecols=range(0,2))\n",
    "b = [df_train['caption'][x] for x in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da27197b-ef5f-403c-9245-e330c6ad21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = clip.tokenize(b,truncate=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607ca7ab-34fb-44f3-b608-4f0514e86d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.perf_counter()\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2fd6f6-2ff1-4b9a-8b68-23c4e86003ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7547964319819584"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9daf88fd-a234-4efe-a4fc-bfb96baa1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05038654-b463-49f4-a4e8-6399dcef56b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8d027-c5bf-4e74-afcc-3995cb1bfb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c11c7a-63e5-4730-9f00-8d2441726b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e67259-1039-49a1-90ac-c483e1627eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/data/cc3m/cc3m_2023/Train_GCC-training.tsv\", sep='\\t', names=[\"caption\",\"url\"], usecols=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92656d0c-90ed-4d17-8f63-a2234345c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [df_train['caption'][x] for x in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d938ef0e-a96b-4fb8-a4bd-1dc571e51f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "inputs = processor(text=b, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ebe63d4-3974-4959-a723-78b3a0c562a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tic = time.perf_counter()\n",
    "    outputs = model.text_model(**inputs)\n",
    "    txt_embeds = outputs[1]\n",
    "    txt_embeds = txt_embeds / txt_embeds.norm(dim=-1, keepdim=True) \n",
    "    toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cdf761c-1530-4f2a-82bf-06ef440a59ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.3069679000182"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e6399d-8a95-496b-8b91-6b3eaa18a4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0479411 ,  0.04968173,  0.25240758, ..., -0.5967182 ,\n",
       "         0.7278716 , -2.205636  ],\n",
       "       [ 0.55611444, -0.7955606 ,  0.9251349 , ...,  0.55774564,\n",
       "        -0.40834782, -0.49836403],\n",
       "       [ 2.6438262 , -0.3794543 , -0.2822747 , ...,  0.32945573,\n",
       "         0.0684305 ,  1.4264344 ],\n",
       "       ...,\n",
       "       [ 2.0603724 , -1.317048  ,  1.0017147 , ...,  0.11916871,\n",
       "         0.4632037 , -0.88777155],\n",
       "       [ 0.39434278, -0.13561863,  2.1092231 , ...,  1.1584276 ,\n",
       "        -0.75113505,  0.5848328 ],\n",
       "       [ 0.32522961, -1.5350556 , -2.4936564 , ..., -1.0410267 ,\n",
       "        -0.02910589,  0.36257917]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc28151a-39ed-4c64-aae3-d3f683a7f18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0436,  0.0021,  0.0105,  ..., -0.0248,  0.0303, -0.0918],\n",
       "        [ 0.0250, -0.0358,  0.0416,  ...,  0.0251, -0.0184, -0.0224],\n",
       "        [ 0.1131, -0.0162, -0.0121,  ...,  0.0141,  0.0029,  0.0610],\n",
       "        ...,\n",
       "        [ 0.0875, -0.0559,  0.0426,  ...,  0.0051,  0.0197, -0.0377],\n",
       "        [ 0.0176, -0.0061,  0.0941,  ...,  0.0517, -0.0335,  0.0261],\n",
       "        [ 0.0133, -0.0627, -0.1019,  ..., -0.0425, -0.0012,  0.0148]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]/outputs[1].norm(dim=-1, keepdim=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f229b1-1721-4eac-b6cf-9a6f9961a522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa12b434-1e2d-4dad-b652-a5538179a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx23_0 = pickle.load(open(\"/data/cc3m_embed/embed_cc3m_2023/order0.p\",\"rb\"))\n",
    "txt23_0 = pickle.load(open(\"/data/cc3m_embed/embed_cc3m_2023/txt0.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2e65a1a-5e2e-41c0-b738-4c3aa724abd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046949618"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "indx = idx23_0.index(2 + i)\n",
    "np.sum(txt23_0[indx]*txt_embeds[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6859bfae-6e8a-4e41-ad30-601194b93fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a55e07c-5599-4935-b287-ae8aa8e12ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6e31354-3d90-4289-8772-173092ab5ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999664"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(txt23_0[indx]*text_features[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6a05d-7692-42bd-8e6d-a098c79cf22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
